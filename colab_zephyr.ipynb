{
    "cells": [
      {
        "cell_type": "markdown",
        "metadata": {},
        "source": [
          "# Text to Knowledge Graph\n",
          "This notebook allows you to preprocess a document, extract relationships using a model, and postprocess the results to generate a knowledge graph."
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "!pip install ollama pandas langchain python-docx pdfplumber"
        ]
      },
      {
        "cell_type": "code",
        "execution_count": null,
        "metadata": {},
        "outputs": [],
        "source": [
          "import os\n",
          "import sys\n",
          "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
          "from docx import Document\n",
          "import pdfplumber\n",
          "import json\n",
          "import pandas as pd\n",
          "from ollama import Client\n",
          "from google.colab import files\n",
          "\n",
          "# 定义预处理函数\n",
          "def read_word_file(file_path):\n",
          "    doc = Document(file_path)\n",
          "    text = \"\"\n",
          "    for para in doc.paragraphs:\n",
          "        if para.text.strip():\n",
          "            text += para.text + \"\\n\"\n",
          "    return text\n",
          "\n",
          "def read_pdf_file(file_path):\n",
          "    with pdfplumber.open(file_path) as pdf:\n",
          "        text = \"\"\n",
          "        for page in pdf.pages:\n",
          "            if page.extract_text():\n",
          "                text += page.extract_text() + \"\\n\"\n",
          "    return text\n",
          "\n",
          "def split_text(text, chunk_size=1000, chunk_overlap=100):\n",
          "    text_splitter = RecursiveCharacterTextSplitter(\n",
          "        chunk_size=chunk_size,\n",
          "        chunk_overlap=chunk_overlap,\n",
          "        separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \"]\n",
          "    )\n",
          "    chunks = text_splitter.split_text(text)\n",
          "    print(f\"切分后的块数：{len(chunks)}\")\n",
          "    return chunks\n",
          "\n",
          "def save_chunks_to_file(chunks, output_file):\n",
          "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
          "        for i, chunk in enumerate(chunks):\n",
          "            f.write(f\"### Chunk {i + 1} ###\\n\")\n",
          "            f.write(chunk.strip())\n",
          "            f.write(\"\\n\\n\")\n",
          "\n",
          "def preprocess_document(file_path, output_file, chunk_size=1000, chunk_overlap=100):\n",
          "    if file_path.endswith(\".docx\"):\n",
          "        text = read_word_file(file_path)\n",
          "    elif file_path.endswith(\".pdf\"):\n",
          "        text = read_pdf_file(file_path)\n",
          "    else:\n",
          "        raise ValueError(\"仅支持 Word (.docx) 和 PDF 文件 (.pdf)\")\n",
          "    chunks = split_text(text, chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
          "    print(f\"文档已切分为 {len(chunks)} 块。\")\n",
          "    save_chunks_to_file(chunks, output_file)\n",
          "    print(f\"切分后的文本已保存到文件：{output_file}\")\n",
          "\n",
          "# 定义处理函数\n",
          "SYS_PROMPT = (\n",
          "    \"你是一个网络图制作者，从给定的上下文中提取术语及其关系。\\n\"\n",
          "    \"你被提供了一个上下文块（由````限定）你的任务是提取所给上下文中提到的术语的本体。\\n\"\n",
          "    \"思路1: 在遍历每个句子时，思考其中提到的关键术语。 术语可能包括对象、实体、位置、组织、人物、任务、首字母缩写词、文档、服务、概念等。\\n\"\n",
          "    \"术语类别: \\n\"\n",
          "    \"人物：如John，Mary。\\n\"\n",
          "    \"地点：如Paris，New York。\\n\"\n",
          "    \"组织：如Google，NASA。\\n\"\n",
          "    \"事件/活动：如会议，讲座。\\n\"\n",
          "    \"物品/物体：如手机，汽车。\\n\"\n",
          "    \"概念/抽象术语：如自由，科技。\\n\"\n",
          "    \"时间/日期：如2022年5月，昨天。\\n\"\n",
          "    \"任务：如进行调查，提交报告。\\n\"\n",
          "    \"文档：如项目报告，研究论文。\\n\"\n",
          "    \"其他术语：如计算机程序，网络协议。\\n\"\n",
          "    \"思路2: 思考这些术语如何与其他术语一对一地关联。 过滤程序在句子或同一段落中提到的术语彼此相关。 术语可以与许多其他术语相相识。\\n\"\n",
          "    \"关系类型：\\n\"\n",
          "    \"拥有：如John owns a car。\\n\"\n",
          "    \"属于：如The book belongs to Mary。\\n\"\n",
          "    \"参与：如Alice participated in the event。\\n\"\n",
          "    \"位置：如Paris is in France。\\n\"\n",
          "    \"所属组织：如John works for Microsoft。\\n\"\n",
          "    \"因果关系：如The rain caused the flood。\\n\"\n",
          "    \"时间关系：如The meeting is scheduled for tomorrow。\\n\"\n",
          "    \"上下文关系：如Mary met John at the event。\\n\"\n",
          "    \"其他复杂关系：如This event happened due to a change in policy。\\n\"\n",
          "    \"思路3: 找出每对术语之间的关系。 对于复杂或模糊的关系，考虑根据上下文推理。例如，如果文本中提到“Mary owns a car”，你可以推测“Mary”和“car”之间存在“拥有”关系。\\n\\n\"\n",
          "    \"跨句和跨段落关系：\\n\"\n",
          "    \"在长文本中，术语的关系可能跨越多个句子或段落。请注意，跨句、跨段落的术语间可能存在直接或间接的关系。\\n\"\n",
          "    \"如果术语在多个句子中提到，并且它们之间有联系，考虑它们可能的关系。\\n\"\n",
          "    \"和它们之间的关系，如下所示：\\n\"\n",
          "    \"歧义处理：\\n\"\n",
          "    \"对于歧义术语，根据上下文来推测其具体含义。\\n\"\n",
          "    \"例如，“Apple”可能指公司，也可能指水果；“Bank”可能指金融机构或河岸。\\n\"\n",
          "    \"将你的输出格式化为 JSON 列表。列表的每个元素包含一对术语和它们之间的关系，如下所示：\\n\"\n",
          "    \"[\\n\"\n",
          "    \"  {\\n\"\n",
          "    \"    \\\"node_1\\\": \\\"术语1\\\",\\n\"\n",
          "    \"    \\\"node_2\\\": \\\"术语2\\\",\\n\"\n",
          "    \"    \\\"edge\\\": \\\"术语1和术语2之间的关系描述\\\"\\n\"\n",
          "    \"  },\\n\"\n",
          "    \"  {\\n\"\n",
          "    \"    \\\"node_1\\\": \\\"术语3\\\",\\n\"\n",
          "        \"    \\\"node_2\\\": \\\"术语4\\\",\\n\"\n",
          "    \"    \\\"edge\\\": \\\"术语3和术语4之间的关系描述\\\"\\n\"\n",
          "    \"  },\\n\"\n",
          "    \"  {...}\\n\"\n",
          "    \"]\"\n",
          ")\n",
          "\n",
          "USER_PROMPT = \"context: ```{input}``` \\n\\n output:\"\n",
          "\n",
          "client = Client(\n",
          "    host='http://192.168.11.69:11434',\n",
          "    headers={\"Content-Type\": \"application/json\"},\n",
          "    use_gpu=True  # 启用 GPU\n",
          ")\n",
          "\n",
          "def process_chunk(client, chunk):\n",
          "    formatted_user_prompt = USER_PROMPT.format(input=chunk)\n",
          "    response = client.chat(model='zephyr', messages=[\n",
          "        {\n",
          "            'role': 'system',\n",
          "            'content': SYS_PROMPT,\n",
          "        },\n",
          "        {\n",
          "            'role': 'user',\n",
          "            'content': formatted_user_prompt,\n",
          "        },\n",
          "    ])\n",
          "    try:\n",
          "        return json.loads(response.message['content'])\n",
          "    except json.JSONDecodeError:\n",
          "        print(\"解析模型响应失败，跳过该块。\")\n",
          "        return []\n",
          "\n",
          "def read_chunks_from_file(file_path):\n",
          "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
          "        chunks = f.read().split(\"### Chunk\")\n",
          "    return [chunk.strip() for chunk in chunks if chunk.strip()]\n",
          "\n",
          "def chunks_to_dataframe(chunks_results):\n",
          "    all_relations = []\n",
          "    for chunk_result in chunks_results:\n",
          "        all_relations.extend(chunk_result)\n",
          "    df = pd.DataFrame(all_relations)\n",
          "    df = df[df['node_1'] != df['node_2']]\n",
          "    return df\n",
          "\n",
          "def process_text(input_chunks_file):\n",
          "    chunks = read_chunks_from_file(input_chunks_file)\n",
          "    print(f\"读取到 {len(chunks)} 个文本块。\")\n",
          "    chunks_results = []\n",
          "    for i, chunk in enumerate(chunks):\n",
          "        print(f\"正在处理第 {i + 1}/{len(chunks)} 块...\")\n",
          "        chunk_result = process_chunk(client, chunk)\n",
          "        chunks_results.append(chunk_result)\n",
          "    final_df = chunks_to_dataframe(chunks_results)\n",
          "    return final_df\n",
          "\n",
          "def save_relations_to_csv(df, output_csv):\n",
          "    df.to_csv(output_csv, index=False, encoding=\"utf-8\")\n",
          "    print(f\"处理完成，结果已保存到 {output_csv}\")\n",
          "\n",
          "def main(input_chunks_file, output_csv):\n",
          "    final_df = process_text(input_chunks_file)\n",
          "    save_relations_to_csv(final_df, output_csv)\n",
          "\n",
          "# 定义后处理函数\n",
          "def postprocess_csv_file(input_file, output_file):\n",
          "    df = pd.read_csv(input_file)\n",
          "    if not {'node_1', 'node_2', 'edge'}.issubset(df.columns):\n",
          "        raise ValueError(\"输入文件必须包含 'node_1', 'node_2', 'edge' 列。\")\n",
          "    df['edge'] = \"contextual proximity\"\n",
          "    df['count'] = df.groupby(['node_1', 'node_2'])['node_2'].transform('count')\n",
          "    df = df.drop_duplicates(subset=['node_1', 'node_2', 'edge'])\n",
          "    df.to_csv(output_file, index=False)\n",
          "    print(f\"处理完成，结果已保存到 {output_file}\")\n",
          "\n",
          "# 上传输入文件\n",
          "uploaded = files.upload()\n",
          "input_file = list(uploaded.keys())[0]\n",
          "output_chunks_file = \"output_chunks.txt\"\n",
          "output_relations_file = \"output_relations.csv\"\n",
          "output_contextual_relations_file = \"contextual_relations.csv\"\n",
          "\n",
          "# 预处理文档\n",
          "preprocess_document(input_file, output_chunks_file)\n",
          "# 调用主函数处理文本块\n",
          "main(output_chunks_file, output_relations_file)\n",
          "# 后处理 CSV 文件\n",
          "postprocess_csv_file(output_relations_file, output_contextual_relations_file)\n",
          "# 下载输出文件\n",
          "files.download(output_contextual_relations_file)\n"
        ]
      }
    ],
    "metadata": {
      "kernelspec": {
        "display_name": "Python 3",
        "language": "python",
        "name": "python3"
      },
      "language_info": {
        "codemirror_mode": {
          "name": "ipython",
          "version": 3
        },
        "file_extension": ".py",
        "mimetype": "text/x-python",
        "name": "python",
        "nbconvert_exporter": "python",
        "pygments_lexer": "ipython3",
        "version": "3.7.12"
      }
    },
    "nbformat": 4,
    "nbformat_minor": 2
  }